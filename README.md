# building-perceptron
## The project


Ce projet vise √† pr√©senter les bases du deep learning, en impl√©mentant un algorithme du perceptron. Pour cela, il est n√©cessaire de comprendre les notions de Machine Learning et de Deepl Learning, toutes deux li√©es √† l'Intelligence Artificielle, et de pouvoir clairement les distinguer.

### D√©finitions

Le Machine Learning (apprentissage automatique), est branche de l'IA qui permet, √† travers des algorithmes et des mod√®les statistiques, d'analyser des donn√©es (ensemble d'apprentissage) pour en apprendre de ces donn√©es, afin d'appliquer le mod√®le d'apprentissage pour prendre des d√©cisions sur d'autres donn√©es non pr√©sente dans l'ensemble d'apprentissage. 

Le Deep Learning quant √† lui, m√™me s'il est une forme de Machine Learning, consiste √† structurer des algorithmes en couches, pour cr√©er un r√©seau de neurones artificiel, capables d'apprendre et de prendre des d√©cisions intelligentes de fa√ßon autonome. On parle de r√©seau de neurones artificiel, car il s'inspire du fonctionnement du cerveau humain.


### Comparaison

A partir de leurs d√©finitions respectives, on peut ressortir des diff√©rences particuli√®res entre ces deux notions. En termes de complexit√©, le Machine Learning fait souvent appel √† des mod√®les simples (regressions, KNN, arbres de d√©cisions, SVM, etc.). Quant au Deepl Learning, il n√©cessite l'utilisation de mod√®les beaucoup plus complexes, due √† la profondeur des r√©seaux de neurones (CNN, GAN, etc.)
De plus, les algorithmes de Deep Learning sont capables d'une s√©lection de variables (features) presque totalement automatique, √† travers la mesure de l'importance de chaque caract√©ristique. La diff√©rence avec le Machine Learning est que dans ce cas, cette s√©lection est souvent bas√©e sur un choix de l'utilisateur. M√™me si ce choix se fait √† partir de m√©todes statistiques, il reste en grande partie subjectif face √† la s√©lection faite en Deep Learning.
Par cons√©quent, les algorithmes de Deep Learning ont besoin de plus gros volumes de donn√©es pour une s√©lection de variables plus pertinente (probl√®me de dimensionalit√©); ce qui repr√©sente aussi une grande diff√©rence avec le Machine Learning.

### Situations o√π il faut privil√©gier l'un ou l'autre

On choisira des m√©thodes de Machine Learning pl√ªtot que celles de Deep Learning lorsqu'on ne dispose pas d'assez de donn√©es, ce qui peut √™tre difficile √† d√©terminer. En g√©n√©ral, l'analyse de donn√©es complexes (en taille et/ou en structure de donn√©es) oriente le travail vers le Deepl Learning, surtout si on dispose d'une puissance de calcul consid√©rable.
De plus, les mod√®les basiques de Machine Learning (r√©gressions par exemple), permettent d'interpr√®ter clairement les r√©sultats en termes d'effet causal des variables explicatives sur la variable cible; ce qui est n'est pas le cas lorsqu'on utilise des mod√®les de Deep Learning. Pour cela, on utilisera des mod√®les de Machine Learning si on cherche √† interpr√®ter le mod√®le d'une fa√ßon causale, et les mod√®les de Deep Learning lorsque l'objectif principal est la s√©lection de caract√©ristiques pertinentes.


Une fois que les notions de Machine Learning et de Deep Learning ont √©t√© √©claircies, nous pr√©sentons trois (03) diff√©rentes applications du Deep Learning:

# Application 1: Traitement du Langage Naturel (Natural Language Processing)

Le Deep Learning est utilis√© dans les t√¢ches de traduction automatique de texte, √† travers l'algorithme RNN (r√©seaux de neurones r√©currents). Le m√©canisme se base sur la capacit√© de l'algorithme √† garder en m√©moire la s√©quence de texte pr√©c√©dente, et √† se baser sur la traduction de cette s√©quence pour traiter la s√©quence suivante. Par exemple, pour traduire une phrase de 10 mots, l'algorithme va en premier parcourir la phrase, et former un vecteur de taille 10 comportant les mots de la phrase. Ensuite, ce vecteur sera utilis√© pour traduire chacun des mots dans la langue cible, en consid√©rant les traductions d√©j√† g√©n√©r√©es des s√©quences pr√©c√©dentes, ainsi que la probabilit√© que le mot suivant ait une certaine signification selon le contexte de la phrase.


# Application 2: Reconnaissance d'objets (avec Google Lens par exemple)

Les r√©seaux de neurones convolutionnels (CNN), dans leur forme am√©lior√©e R-CNN (regions with CNN features), sont utilis√©es dans la d√©tection d'objets. Pour une image en entr√©e, √† l'entra√Ænement, la premi√®re √©tape est d'utiliser un algorithme pour g√©n√©rer environs 2000 r√©gions ayant une forte probabilit√© de contenir les objets recherch√©s. Ensuite, dans une √©tape de classification, les r√©gions d√©coup√©es sont redimensionn√©es, et le CNN est utilis√© pour extraire les features (caract√©ristiques des objets recherch√©s) de l'image initiale enti√®re. On utilise ensuite un SVM pour classer chaque r√©gion dans l'une des cat√©gories d'objets pr√©d√©finies (un SVM par cat√©gorie), et un BBR (Bounding Box Regressor) pour pr√©dire et affiner les coordonn√©es de chaque r√©gions selon les caract√©ristiques s√©lectionn√©es.

# Application 3: NotebookLM de Google

Cet outil d'IA de Google permet de faire des recherches dans des documents en d√©tectant le texte dans des images. Apr√®s un pr√©traitement des images visant √† supprimer le bruit et √† am√©liorer le texte, un r√©seau de neurones convolutionnel (CNN) est utilis√© pour extraire le texte des images. Ce texte est ensuite analys√© et index√© pour permettre de retrouver plus rapidement les r√©ponses aux questions pos√©es par l'utilisateur.

En s'int√©ressant particuli√®rement au perceptron dans le cadre de ce projet, voici quelques √©l√©ments permettant de comprendre son fonctionnement.

# D√©finition du perceptron et lien avec le neurone biologique

Le perceptron est un mod√®le math√©matique inspir√© par le fonctionnement du neurone biologique, introduit par Frank Rosenblatt en 1957, et consid√©r√© comme le pr√©curseur des r√©seaux de neurones profonds. Il fonctionne comme un neurone biologique, car il re√ßoit une information en entr√©e (valeurs num√©riques), et produit une sortie unique; comme le neurone biologique qui re√ßoit des signaux par les dendrites, et emet un signal par l'axone. De plus, l'information en entr√©e a un poids dans les deux cas: pour le neurone biologique, il s'agit de l'intensit√© de la connexion entre les synapses de deux ou plusieurs neurones connect√©s; le perceptron quant √† lui, assigne des poids √† caque entr√©e, repr√©sentant l'importance de cette entr√©e dans la r√©ponse en sortie. Enfin, un neurone biologique ne d√©clenche un signal que si le potentiel √©lectrique accumul√© d√©passe un certain seuil, ce qui est tr√®s semblable au perceptron qui, √† travers une fonction seuil (fonction d'activation), d√©termine la sortie binaire.

# Fonction mat√©matique du perceptron

La fonction math√©matique du perceptron est: y = f(‚àë(wi * xi) + b, avec:
- y la sortie du perceptron (0 ou 1)
- f() la fonction d'activation
- wi le poids associ√© √† la i-√®me entr√©e
- xi la valeur de la i-√®me entr√©e
- b le biais
 Le biais est √† diff√©rencier du seuil d'activation, m√™me s'ils repr√©sentent tous deux une valeur constante ajout√©e √† la somme pond√©r√©e des entr√©es. En effet, le biais est un param√®tre du mod√®le qui peut √™tre appris pendant l'entra√Ænement, alors que le seuil d'activation est pr√©d√©termin√©. 

 En ce qui concerne l'usage du perceptron, il permet de classer des donn√©es en deux cat√©gories; par exemple, il peut √™tre utilis√© pour la d√©tection de spams, la reconnaissance d'images, et le traitement du langage naturel (analyse de sentiment positif ou n√©gatif dans un discours).

 # R√®gles d'apprentissage du perceptron

 - Apprentissage par Renforcement Hebbien (Perceptron √† seuil): Pour une entr√©e Ak = (xk, ≈∑k), mise √† jour des poids et du seuil en fonction de la diff√©rence entre la sortie calcul√©e y et la sortie d√©sir√©e ≈∑; 
wi_nouveau = wi_ancien + Œª * (≈∑k - yk) * xik
ùúó_nouveau = ùúó_ancien + Œª * (≈∑k - yk) avec:
Œª le taux d'apprentissage, un param√®tre positif qui contr√¥le l'ampleur de la correction;
xik est la valeur de la i-√®me composante du vecteur d'entr√©e xk.
  R√®gle de d√©cision:
    - si la sortie d√©sir√©e est 1 et la sortie calcul√©e est 0, les poids associ√©s aux entr√©es positives sont augment√©s et le seuil est diminu√©;
    - si la sortie d√©sir√©e est 0 et la sortie calcul√©e est 1, les poids associ√©s aux entr√©es positives sont diminu√©s et le seuil est augment√©.

- Apprentissage par Descente du Gradient (Perceptron Continu)
Pour les perceptrons continus utilisant une fonction d'activation diff√©rentiable, l'apprentissage se fait en minimisant une fonction objectif (E) qui mesure l'erreur entre la sortie du perceptron et la sortie d√©sir√©e pour tous les exemples de l'ensemble d'apprentissage. La m√©thode de descente du gradient est utilis√©e pour trouver les valeurs des poids et du seuil qui minimisent cette fonction.
Fonction Objectif : E(w, ùúó) = (1/2) * ‚àë(yk - ≈∑k)^2
o√π la somme s'effectue sur tous les exemples de l'ensemble d'apprentissage.

Formules de mise √† jour :
wi_t+1 = wi_t - Œª * ‚àÇE/‚àÇwi
ùúó_t+1 = ùúó_t - Œª * ‚àÇE/‚àÇùúó
o√π :
t repr√©sente l'it√©ration actuelle.
‚àÇE/‚àÇwi et ‚àÇE/‚àÇùúó repr√©sentent les d√©riv√©es partielles de la fonction objectif par rapport aux poids et au seuil, respectivement.
La m√©thode ajuste les poids et le seuil dans la direction oppos√©e au gradient de la fonction objectif. Cela permet de converger vers un minimum de la fonction objectif, ce qui correspond √† une minimisation de l'erreur de classification.


# Fonction d'activation classique

Il s'agit de la fonction sigmo√Øde, d√©finit comme suit: f(x) = 1 / (1 + e^(-x))

o√π x est la somme des produits des poids des connexions entre les entr√©es et la sortie, et e est la base naturelle (‚âà 2.71828).

Elle prend les valeurs 0 ou 1 selon les entr√©es du perceptron.

# Processus d'entra√Ænement du perceptron

Il se d√©roule en sept (07) √©tapes:
a- initialisation des poids et du seuil avec des valeurs al√©atoires ou pr√©d√©finies;

b- introduction d'un exemple d'apprentissage compos√© d'un vecteur d'entr√©e (xk) et de la sortie d√©sir√©e (≈∑k);

c- calcul de la sortie potentielle (Œæk) en effectuant la somme pond√©r√©e des entr√©es et en y ajoutant le seuil d'activation:
Œæk = ‚àë (wi * xik) + ùúó; la sortie du perceptron (yk) est ensuite d√©termin√©e en appliquant une fonction d'activation au potentiel;

d- comparaison de la sortie du perceptron yk et de celle de d√©part ≈∑k;

e- mise √† jour des poids et du seuil si la sortie calcul√©e est diff√©rente de la sortie d√©sir√©e, dans le but de rapprocher la sortie calcul√©e de la sortie attendue;

f- r√©p√©tition des √©tapes b √† e : les √©tapes d'introduction d'un exemple, de calcul de la sortie, de comparaison et de mise √† jour des poids sont r√©p√©t√©es pour tous les exemples de l'ensemble d'apprentissage;

g- crit√®re d'arr√™t: Le processus d'apprentissage se poursuit jusqu'√† ce qu'un crit√®re d'arr√™t soit atteint; cela peut √™tre:
  - tous les exemples correctement classifi√©s: le perceptron a appris √† classifier correctement tous les exemples de l'ensemble d'apprentissage;
  - nombre maximal d'it√©rations atteint: le processus d'apprentissage s'arr√™te apr√®s un nombre pr√©d√©fini d'it√©rations, m√™me si tous les exemples ne sont pas correctement classifi√©s;
  - gradient de la fonction objectif inf√©rieur √† une valeur seuil: pour les perceptrons continus, l'apprentissage s'arr√™te lorsque le gradient de la fonction objectif est suffisamment petit, indiquant que l'erreur de classification est minimis√©e.

# Limites du perceptron

a- S√©parabilit√© lin√©aire: le perceptron ne peut classifier correctement que des ensembles de donn√©es lin√©airement s√©parables. Cela signifie qu'il est incapable de trouver une solution pour des probl√®mes o√π les classes ne peuvent pas √™tre divis√©es par une ligne droite (en deux dimensions) ou un hyperplan (en dimensions sup√©rieures);

b- Classification binaire: le perceptron ne peut √™tre utilis√© que pour des t√¢ches de classification binaire; au-del√† de deux classes, il faut un r√©seau avec plusieurs couches de neurones;

c- Donn√©es ind√©pendantes et identiquement distribu√©es (i.i.d): l'utilisation du perceptron suppose que les donn√©es sont ind√©pendantes et identiquement distribu√©es, ce qui signifie que chaque point de donn√©es est ind√©pendant des autres et provient de la m√™me distribution de probabilit√©; cette hypoth√®se peut ne pas √™tre valable dans de nombreux sc√©narios r√©els, o√π les donn√©es peuvent pr√©senter des d√©pendances et provenir de distributions diff√©rentes.

d- Difficult√© √† g√©rer les donn√©es avec des bruits ou avec desvaleurs aberrantes: le perceptron est sensible aux donn√©es bruit√©es, c'est-√†-dire aux donn√©es contenant des erreurs ou des incoh√©rences; les exemples erron√©s peuvent perturber l'apprentissage et emp√™cher le perceptron de trouver une solution optimale.

e- Sensibilit√© aux variations de poids et au choix de la fonction d'activation: utiliser par exemple des poids al√©atoires

f- Surapprentissage: le perceptron continue d'apprendre m√™me lorsque les donn√©es sont trait√©es et enregistr√©es;  cela peut engendrer des difficult√©s li√©es √† une suradaptation et √† une surmod√©lisation.



### Project Members
Yann Sasse
Amina Sadio
Julien Ract-Mugnerot

### Requirements

python pandas etc


## Note √† l'√©quipe:

M√©thode pour importer un fichier sur une autre branche:
aller sur la branche vers laquelle l'on veut copier le fichier
git checkout NomBrancheOuEstLeFichier -- file(avec extensions)



- Id√©alement on ne va travailler que sur deux branches principales et le moins de fichiers possibles, √©vitons les branches julien et julien_notebook.ipynb
  - Class_Perceptron
  - Data_Exploration --> Data_Preprocessing --> Feature_Selection ...
`Ce serait bien qu'on change de branche √† la compl√©tion de chaque √©tape sur la partie notebook.`


- R√©aliser les commentaires associ√©s au notebook en m√™me temps que chaque √©tape produite
- Si les push √† chaque fonctionnalit√© peuvent suivrent la forme "[Create|Update|Delete] Data Import" recommand√© par Kawther l'ann√©e derni√®re, ce serait top.
- Personellement j'aimerais beaucoup qu'on utilise activement le kanban github pour savoir ou on en est
- R√©aliser la classe √† partir de ce tuto pour d√©marrer: https://www.youtube.com/watch?v=-KLnurhX-Pg
- absolument utiliser Boruta pour la feature selection, si on a le temps chercher d'autres technique
- R√©alisation de multiples tests unitaires pour la classe python
